{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import files\n",
    "import os\n",
    "import numpy as np\n",
    "#get titles\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "moviehtmldir = './movie/'\n",
    "moviedict = {}\n",
    "for filename in [f for f in os.listdir(moviehtmldir) if f[0]!='.']:\n",
    "    id = filename.split('.')[0]\n",
    "    f = open(moviehtmldir+'/'+filename)\n",
    "    parsed_html = BeautifulSoup(f.read())\n",
    "    try:\n",
    "       title = parsed_html.body.h1.text\n",
    "       \n",
    "    except:\n",
    "       title = 'none'\n",
    "    moviedict[id] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ListDocs(dirname):\n",
    "    docs = []\n",
    "    titles = []\n",
    "    for filename in [f for f in os.listdir(dirname) if str(f)[0]!='.']:\n",
    "        f = open(dirname+'/'+filename,'r')\n",
    "        id = filename.split('.')[0].split('_')[1]\n",
    "        titles.append(moviedict[id])\n",
    "        docs.append(f.read())\n",
    "    return docs,titles\n",
    "\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "pos_textreviews,pos_titles = ListDocs(dir+'pos/')\n",
    "neg_textreviews,neg_titles = ListDocs(dir+'neg/')\n",
    "tot_textreviews = pos_textreviews+neg_textreviews\n",
    "tot_titles = pos_titles+neg_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package 'stopwords' to\n",
      "[nltk_data]     /Users/andrea/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tknzr = WordPunctTokenizer()\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stoplist = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def PreprocessReviews(text,stop=[],stem=False):\n",
    "    #print profile\n",
    "    words = tknzr.tokenize(text)\n",
    "    if stem:\n",
    "       words_clean = [stemmer.stem(w) for w in [i.lower() for i in words if i not in stop]]\n",
    "    else:\n",
    "       words_clean = [i.lower() for i in words if i not in stop]\n",
    "    return words_clean\n",
    "\n",
    "Review = namedtuple('Review','words title tags')\n",
    "dir = './review_polarity/txt_sentoken/'\n",
    "do2vecstem = True\n",
    "reviews_pos = []\n",
    "cnt = 0\n",
    "for filename in [f for f in os.listdir(dir+'pos/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'pos/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_pos.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['pos_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "    \n",
    "reviews_neg = []\n",
    "cnt= 0\n",
    "for filename in [f for f in os.listdir(dir+'neg/') if str(f)[0]!='.']:\n",
    "    f = open(dir+'neg/'+filename,'r')\n",
    "    id = filename.split('.')[0].split('_')[1]\n",
    "    reviews_neg.append(Review(PreprocessReviews(f.read(),stoplist,do2vecstem),moviedict[id],['neg_'+str(cnt)]))\n",
    "    cnt+=1\n",
    "\n",
    "tot_reviews = reviews_pos + reviews_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pos_0'], ['pos_1'], ['pos_2'], ['pos_3'], ['pos_4']]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "#shuffle(tot_reviews)\n",
    "#test\n",
    "print [r.tags for r in tot_reviews[:5]]\n",
    "#flatten \n",
    "tot_poswords = [val for l in [r.words for r in reviews_pos] for val in l]\n",
    "tot_negwords = [val for l in [r.words for r in reviews_neg] for val in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "#train on 80%\n",
    "portion = int(len(tot_reviews)*0.8)\n",
    "print portion\n",
    "train_reviews = tot_reviews[:portion]\n",
    "test_reviews = tot_reviews[portion:]\n",
    "print len(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#split in test training sets\n",
    "def word_features(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "negfeatures = [(word_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(word_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionneg] + posfeatures[:portionpos]\n",
    "print len(trainfeatures)\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "#shuffle(testfeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "#training naive bayes \n",
    "\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2825\n"
     ]
    }
   ],
   "source": [
    "##testing\n",
    "\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util, nltk.metrics\n",
    "\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "word_fd = FreqDist()\n",
    "label_word_fd = ConditionalFreqDist()\n",
    " \n",
    "for word in tot_poswords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['pos'][word.lower()] +=1\n",
    " \n",
    "for word in tot_negwords:\n",
    "    word_fd[word.lower()] +=1\n",
    "    label_word_fd['neg'][word.lower()] +=1\n",
    "pos_words = len(tot_poswords)\n",
    "neg_words = len(tot_negwords)\n",
    "\n",
    "tot_words = pos_words + neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of w in pos:  6193\n",
      "num of w in pos+neg  11199\n",
      "num pos  379022\n",
      "27.362859883\n",
      "27.362859883\n"
     ]
    }
   ],
   "source": [
    "w = 'film'\n",
    "print 'num of w in pos: ',label_word_fd['pos'][w]\n",
    "print 'num of w in pos+neg ',word_fd[w]\n",
    "print 'num pos ',pos_words\n",
    "print BigramAssocMeasures.chi_sq(label_word_fd['pos'][w],\n",
    "                (word_fd[w], pos_words), tot_words)\n",
    "\n",
    "def _contingency(n_ii, (n_ix, n_xi), n_xx):\n",
    "        \"\"\"Calculates values of a bigram contingency table from marginal values.\"\"\"\n",
    "        n_oi = n_xi - n_ii\n",
    "        n_io = n_ix - n_ii\n",
    "        return (n_ii, n_oi, n_io, n_xx - n_ii - n_oi - n_io)\n",
    "'''\n",
    "00169     def _contingency(n_ii, (n_ix, n_xi), n_xx):\n",
    "        \"\"\"Calculates values of a bigram contingency table from marginal values.\"\"\"\n",
    "        n_oi = n_xi - n_ii\n",
    "        n_io = n_ix - n_ii\n",
    "        return (n_ii, n_oi, n_io, n_xx - n_ii - n_oi - n_io)\n",
    "00189     def phi_sq(cls, *marginals):\n",
    "        \"\"\"Scores bigrams using phi-square, the square of the Pearson correlation\n",
    "        coefficient.\n",
    "        \"\"\"\n",
    "        n_ii, n_io, n_oi, n_oo = cls._contingency(*marginals)\n",
    "\n",
    "        return (float((n_ii*n_oo - n_io*n_oi)**2) /\n",
    "                ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))\n",
    "\n",
    "    @classmethod\n",
    "00199     def chi_sq(cls, n_ii, (n_ix, n_xi), n_xx):\n",
    "        \"\"\"Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
    "        of bigrams, as in Manning and Schutze 5.3.3.\n",
    "        \"\"\"\n",
    "        return n_xx * cls.phi_sq(n_ii, (n_ix, n_xi), n_xx)\n",
    "\n",
    "'''\n",
    "n_xx = tot_words\n",
    "n_ii =  label_word_fd['pos'][w]\n",
    "n_ix = word_fd[w]\n",
    "n_xi = pos_words\n",
    "n_io = n_ix\n",
    "n_oi = n_xi\n",
    "n_oo = n_xx\n",
    "n_ii, n_io, n_oi, n_oo =_contingency(n_ii, (n_ix, n_xi), n_xx)\n",
    "print n_xx * (float((n_ii*n_oo - n_io*n_oi)**2) /\n",
    "                ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot:  26082\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "#select the best words in terms of information contained in the two classes pos and neg\n",
    "word_scores = {}\n",
    " \n",
    "for word, freq in word_fd.iteritems():\n",
    "    pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n",
    "                (freq, pos_words), tot_words)\n",
    "    neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n",
    "                (freq, neg_words), tot_words)\n",
    "    word_scores[word] = pos_score + neg_score\n",
    "print 'tot: ',len(word_scores)\n",
    "best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:10000]\n",
    "bestwords = set([w for w, s in best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#training naive bayes with chi square feature selection of best words\n",
    "def best_words_features(words):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    "\n",
    "negfeatures = [(best_words_features(r.words), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(best_words_features(r.words), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.1275\n"
     ]
    }
   ],
   "source": [
    "##test with feature chi square selection\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 - 800\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "#train bigram:\n",
    "def bigrams_words_features(words, nbigrams=200,measure=BigramAssocMeasures.chi_sq):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = bigram_finder.nbest(measure, nbigrams)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "negfeatures = [(bigrams_words_features(r.words,500), 'neg') for r in reviews_neg]\n",
    "posfeatures = [(bigrams_words_features(r.words,500), 'pos') for r in reviews_pos]\n",
    "portionpos = int(len(posfeatures)*0.8)\n",
    "portionneg = int(len(negfeatures)*0.8)\n",
    "print portionpos,'-',portionneg\n",
    "trainfeatures = negfeatures[:portionpos] + posfeatures[:portionneg]\n",
    "print len(trainfeatures)\n",
    "classifier = NaiveBayesClassifier.train(trainfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ciao', 'mamma'), ('come', 'mi')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ciao': True,\n",
       " 'come': True,\n",
       " 'diverto': True,\n",
       " 'guarda': True,\n",
       " 'mamma': True,\n",
       " 'mi': True,\n",
       " ('ciao', 'mamma'): True,\n",
       " ('come', 'mi'): True}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_bigram_word_features(words, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "            bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "            bigrams = bigram_finder.nbest(score_fn, n)\n",
    "            print bigrams\n",
    "            return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "test_bigram_word_features(\"ciao mamma guarda come mi diverto\".split(' '),BigramAssocMeasures.chi_sq,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test on:  400\n",
      "error rate:  0.2\n"
     ]
    }
   ],
   "source": [
    "##test bigram\n",
    "testfeatures = negfeatures[portionneg:] + posfeatures[portionpos:]\n",
    "shuffle(testfeatures)\n",
    "err = 0\n",
    "print 'test on: ',len(testfeatures)\n",
    "for r in testfeatures:\n",
    "    sent = classifier.classify(r[0])\n",
    "    #print r[1],'-pred: ',sent\n",
    "    if sent != r[1]:\n",
    "       err +=1.\n",
    "print 'error rate: ',err/float(len(testfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fa1d85ff9043>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenSimCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_textreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mdict_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_lda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# if a training corpus was provided, start estimating the model right away\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/models/ldamodel.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0mreallen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m                 \u001b[0mreallen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keep track of how many documents we've processed so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36mchunkize_serial\u001b[0;34m(iterable, chunksize, as_numpy)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;31m# convert each document to a 2d numpy array (~6x faster when transmitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# chunk data over the wire, in Pyro)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-fa1d85ff9043>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m            \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoplist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                    \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m            \u001b[0;32mdef\u001b[0m \u001b[0miter_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoplist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/gensim/corpora/dictionary.pyc\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LDA\n",
    "import gensim.models\n",
    "from gensim import models\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tknzr = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "class GenSimCorpus(object):\n",
    "           def __init__(self, texts, stoplist=[],bestwords=[],stem=False):\n",
    "               self.texts = texts\n",
    "               self.stoplist = stoplist\n",
    "               self.stem = stem\n",
    "               self.bestwords = bestwords\n",
    "               self.dictionary = gensim.corpora.Dictionary(self.iter_docs(texts, stoplist))\n",
    "            \n",
    "           def __len__(self):\n",
    "               return len(self.texts)\n",
    "           def __iter__(self):\n",
    "               for tokens in self.iter_docs(self.texts, self.stoplist):\n",
    "                   yield self.dictionary.doc2bow(tokens)\n",
    "           def iter_docs(self,texts, stoplist):\n",
    "               for text in texts:\n",
    "                   if self.stem:\n",
    "                      yield (stemmer.stem(w) for w in [x for x in tknzr.tokenize(text) if x not in stoplist])\n",
    "                   else:\n",
    "                      if len(bestwords)>0:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x in bestwords)\n",
    "                      else:\n",
    "                         yield (x for x in tknzr.tokenize(text) if x not in stoplist)            \n",
    "            \n",
    "num_topics = 10\n",
    "corpus = GenSimCorpus(tot_textreviews, stoplist,[],False)\n",
    "dict_lda = corpus.dictionary\n",
    "lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lda,passes=10, iterations=50)\n",
    "print lda.show_topics(num_topics=num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0   words:  0.069*in + 0.032*film + 0.022*on + 0.022*who + 0.018*not + 0.017*be + 0.013*out + 0.011*like + 0.011*there + 0.007*also + 0.007*even + 0.007*much + 0.007*would + 0.007*other + 0.006*see + 0.006*life + 0.006*get + 0.006*scene + 0.006*way + 0.005*first + 0.005*well + 0.005*off + 0.005*john + 0.005*director + 0.004*know + 0.004*could + 0.004*best + 0.004*go + 0.004*great + 0.004*us + 0.004*new + 0.004*doesn + 0.003*world + 0.003*role + 0.003*take + 0.003*say + 0.003*plot + 0.003*still + 0.003*bad + 0.003*year\n",
      "\n",
      "topic  1   words:  0.069*in + 0.032*film + 0.026*be + 0.022*on + 0.020*not + 0.017*who + 0.012*there + 0.012*like + 0.011*out + 0.009*even + 0.008*would + 0.007*bad + 0.007*well + 0.007*much + 0.006*plot + 0.006*get + 0.006*action + 0.006*could + 0.006*way + 0.006*other + 0.006*off + 0.005*first + 0.005*also + 0.005*life + 0.005*doesn + 0.005*see + 0.004*re + 0.004*best + 0.004*made + 0.004*scene + 0.004*new + 0.004*director + 0.004*great + 0.004*love + 0.004*know + 0.003*us + 0.003*isn + 0.003*almost + 0.003*think + 0.003*may\n",
      "\n",
      "topic  2   words:  0.059*in + 0.032*film + 0.025*on + 0.019*be + 0.017*not + 0.013*who + 0.012*there + 0.012*like + 0.010*out + 0.009*first + 0.008*even + 0.007*would + 0.007*see + 0.007*well + 0.007*also + 0.006*know + 0.006*alien + 0.006*much + 0.006*get + 0.006*other + 0.005*off + 0.005*new + 0.005*bad + 0.005*plot + 0.004*could + 0.004*re + 0.004*best + 0.004*last + 0.004*world + 0.004*horror + 0.004*life + 0.004*still + 0.004*2 + 0.004*go + 0.004*made + 0.004*way + 0.003*doesn + 0.003*planet + 0.003*seen + 0.003*thing\n",
      "\n",
      "topic  3   words:  0.064*in + 0.025*film + 0.021*on + 0.019*who + 0.017*be + 0.016*not + 0.015*there + 0.011*out + 0.010*like + 0.008*even + 0.007*also + 0.006*much + 0.006*big + 0.006*other + 0.006*first + 0.006*get + 0.006*would + 0.005*way + 0.005*plot + 0.005*scene + 0.005*see + 0.004*life + 0.004*off + 0.004*well + 0.004*doesn + 0.004*love + 0.004*re + 0.004*go + 0.004*great + 0.003*bad + 0.003*action + 0.003*could + 0.003*director + 0.003*us + 0.003*better + 0.003*black + 0.003*new + 0.003*still + 0.003*take + 0.003*know\n",
      "\n",
      "topic  4   words:  0.076*in + 0.026*on + 0.025*film + 0.017*who + 0.015*be + 0.014*not + 0.012*like + 0.010*out + 0.009*there + 0.008*even + 0.006*other + 0.006*get + 0.006*also + 0.006*well + 0.006*much + 0.005*would + 0.005*best + 0.005*way + 0.005*new + 0.005*see + 0.005*life + 0.005*love + 0.004*great + 0.004*first + 0.004*director + 0.004*off + 0.004*world + 0.004*doesn + 0.004*plot + 0.004*still + 0.004*could + 0.004*scene + 0.004*own + 0.004*us + 0.003*action + 0.003*made + 0.003*enough + 0.003*go + 0.003*big + 0.003*town\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_topics):\n",
    "    print 'topic ',t,'  words: ',lda.print_topic(t,topn=40)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#tfidf wrong lda takes only integers NOT float as tfidf!!!\n",
    "\n",
    "#tfidf = models.TfidfModel(corpus)\n",
    "#corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "#filter out very common words like mobie and film or very unfrequent terms\n",
    "out_ids = [tokenid for tokenid, docfreq in dict_lda.dfs.iteritems() if docfreq > 1000 or docfreq < 3 ]\n",
    "dict_lfq = copy.deepcopy(dict_lda)\n",
    "dict_lfq.filter_tokens(out_ids)\n",
    "dict_lfq.compactify()\n",
    "corpus = [dict_lfq.doc2bow(tknzr.tokenize(text)) for text in tot_textreviews]\n",
    "#docfreq >500 and alpha=0.01,eta=0.0000001\n",
    "#docfreq >500 and alpha=0.01,eta=0.01\n",
    "lda_lfq = models.LdaModel(corpus, num_topics=num_topics, id2word=dict_lfq,passes=10, iterations=50,alpha=0.01,eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic  0   words:  0.009*best + 0.008*life + 0.008*although + 0.008*great + 0.007*director + 0.006*own + 0.006*see + 0.006*town + 0.006*doesn + 0.005*still\n",
      "\n",
      "topic  1   words:  0.014*see + 0.010*know + 0.008*bad + 0.008*off + 0.008*think + 0.007*plot + 0.007*could + 0.007*re + 0.007*life + 0.007*m\n",
      "\n",
      "topic  2   words:  0.011*disney + 0.009*off + 0.009*action + 0.009*plot + 0.008*love + 0.008*life + 0.007*wild + 0.007*could + 0.006*mulan + 0.006*new\n",
      "\n",
      "topic  3   words:  0.009*scene + 0.008*life + 0.007*new + 0.007*know + 0.007*doesn + 0.007*off + 0.007*could + 0.006*bad + 0.006*director + 0.006*see\n",
      "\n",
      "topic  4   words:  0.014*truman + 0.009*life + 0.009*best + 0.008*doesn + 0.007*scene + 0.007*own + 0.007*world + 0.007*sandler + 0.007*see + 0.006*new\n",
      "\n",
      "topic  5   words:  0.009*bad + 0.008*big + 0.008*off + 0.007*plot + 0.007*doesn + 0.007*director + 0.007*scene + 0.007*go + 0.006*see + 0.006*better\n",
      "\n",
      "topic  6   words:  0.013*plot + 0.012*action + 0.012*alien + 0.011*bad + 0.009*new + 0.008*off + 0.008*planet + 0.008*see + 0.007*could + 0.006*scene\n",
      "\n",
      "topic  7   words:  0.013*action + 0.009*plot + 0.007*war + 0.007*off + 0.007*see + 0.007*re + 0.007*van + 0.006*director + 0.006*great + 0.006*made\n",
      "\n",
      "topic  8   words:  0.012*love + 0.009*best + 0.008*see + 0.007*could + 0.007*life + 0.006*new + 0.006*scene + 0.006*off + 0.006*go + 0.006*re\n",
      "\n",
      "topic  9   words:  0.016*life + 0.010*world + 0.007*scene + 0.007*could + 0.006*mother + 0.006*own + 0.006*love + 0.006*role + 0.006*off + 0.006*father\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(num_topics):\n",
    "    print 'topic ',t,'  words: ',lda_lfq.print_topic(t,topn=10)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#topics for each doc\n",
    "def GenerateDistrArrays(corpus):\n",
    "         for i,dist in enumerate(corpus[:10]):\n",
    "             dist_array = np.zeros(num_topics)\n",
    "             for d in dist:\n",
    "                 dist_array[d[0]] =d[1]\n",
    "             if dist_array.argmax() == 6 :\n",
    "                print tot_titles[i]\n",
    "corpus_lda = lda_lfq[corpus]\n",
    "GenerateDistrArrays(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n",
      "epoch 10\n",
      "epoch 11\n",
      "epoch 12\n",
      "epoch 13\n",
      "epoch 14\n",
      "epoch 15\n",
      "epoch 16\n",
      "epoch 17\n",
      "epoch 18\n",
      "epoch 19\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "shuffle(tot_reviews)\n",
    "cores = multiprocessing.cpu_count()\n",
    "vec_size = 500\n",
    "model_d2v = Doc2Vec(dm=1, dm_concat=0, size=vec_size, window=5, negative=0, hs=0, min_count=1, workers=cores)\n",
    "\n",
    "#build vocab\n",
    "model_d2v.build_vocab(tot_reviews)\n",
    "#train\n",
    "numepochs= 20\n",
    "for epoch in range(numepochs):\n",
    "    try:\n",
    "        print 'epoch %d' % (epoch)\n",
    "        model_d2v.train(tot_reviews)\n",
    "        model_d2v.alpha *= 0.99\n",
    "        model_d2v.min_alpha = model_d2v.alpha\n",
    "    except (KeyboardInterrupt, SystemExit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 - 400\n"
     ]
    }
   ],
   "source": [
    "#split train,test sets\n",
    "trainingsize = 2*int(len(reviews_pos)*0.8)\n",
    "\n",
    "train_d2v = np.zeros((trainingsize, vec_size))\n",
    "train_labels = np.zeros(trainingsize)\n",
    "test_size = len(tot_reviews)-trainingsize\n",
    "test_d2v = np.zeros((test_size, vec_size))\n",
    "test_labels = np.zeros(test_size)\n",
    "\n",
    "#print len(train_d2v),'-',len(test_d2v)\n",
    "cnt_train = 0\n",
    "cnt_test = 0\n",
    "for r in reviews_pos:\n",
    "    name_pos = r.tags[0]\n",
    "    if int(name_pos.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_pos]\n",
    "        test_labels[cnt_test] = 1\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_pos]\n",
    "        train_labels[cnt_train] = 1\n",
    "        cnt_train +=1\n",
    "\n",
    "for r in reviews_neg:\n",
    "    name_neg = r.tags[0]\n",
    "    if int(name_neg.split('_')[1])>= int(trainingsize/2.):\n",
    "        test_d2v[cnt_test] = model_d2v.docvecs[name_neg]\n",
    "        test_labels[cnt_test] = 0\n",
    "        cnt_test +=1\n",
    "    else:\n",
    "        train_d2v[cnt_train] = model_d2v.docvecs[name_neg]       \n",
    "        train_labels[cnt_train] = 0\n",
    "        cnt_train +=1\n",
    "\n",
    "print cnt_train,'-',cnt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5175\n",
      "0.72125\n",
      "0.5225\n",
      "0.5075\n"
     ]
    }
   ],
   "source": [
    "#train log regre\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_d2v, train_labels)\n",
    "print classifier.score(test_d2v,test_labels)\n",
    "print classifier.score(train_d2v, train_labels)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print clf.score(test_d2v,test_labels)\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(train_d2v, train_labels)\n",
    "print clf.score(test_d2v,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
